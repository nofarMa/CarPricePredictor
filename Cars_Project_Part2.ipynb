{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31dcedd2-3165-422d-aef6-3586a6a954cc",
   "metadata": {},
   "source": [
    "# Nofar Mahrabi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d4813c41-e921-47ed-b709-d507c8fd0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.impute import SimpleImputer \n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063e0f4-2bef-4ed8-85c7-735fbb81a654",
   "metadata": {},
   "source": [
    "### Functions in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "466a15a9-4fa2-4ff0-9bc0-cb6a923c3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_column_distribution(df, column_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df[column_name], bins=30, kde=True)\n",
    "    plt.title(f'Distribution of {column_name}')\n",
    "    plt.xlabel(column_name)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3365860-c1b2-4635-981d-ceb5b0b6db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_supply_score_knn(df, num_features, cat_features, n_neighbors=8):\n",
    "    # Encode categorical features using One-Hot Encoding\n",
    "    encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "    encoded_categorical = encoder.fit_transform(df[cat_features])\n",
    "    encoded_categorical_df = pd.DataFrame(encoded_categorical, columns=encoder.get_feature_names_out(cat_features))\n",
    "    \n",
    "    # Combine numerical and encoded categorical features\n",
    "    df_combined = pd.concat([df[num_features], encoded_categorical_df], axis=1)\n",
    "    \n",
    "    # Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_combined)\n",
    "    \n",
    "    # Add the Supply_score column\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=df_combined.columns)\n",
    "    df_scaled['Supply_score'] = df['Supply_score']\n",
    "    \n",
    "    # Use KNNImputer to fill missing values\n",
    "    imputer = KNNImputer(n_neighbors=n_neighbors)\n",
    "    df_imputed = imputer.fit_transform(df_scaled)\n",
    "    df_imputed = pd.DataFrame(df_imputed, columns=df_scaled.columns)\n",
    "    \n",
    "    # Update the original DataFrame with the filled Supply_score values\n",
    "    df['Supply_score'] = df_imputed['Supply_score']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43a1374a-19f1-4135-9c6a-6091f9b3af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_model_column(model, manufactor):\n",
    "    if not isinstance(model, str):\n",
    "        return None\n",
    "    # Remove special characters\n",
    "    model = re.sub(r'[^\\w\\s]', '', model)\n",
    "    # Remove extra spaces\n",
    "    model = re.sub(r'\\s+', ' ', model).strip()\n",
    "    # Remove manufacturer name\n",
    "    manufactor = re.sub(r'[^\\w\\s]', '', manufactor)\n",
    "    model = re.sub(fr'\\b{manufactor}\\b', '', model, flags=re.IGNORECASE).strip()\n",
    "    # Remove years\n",
    "    model = re.sub(r'\\b\\d{4}\\b', '', model).strip()\n",
    "    # If the cleaned string is empty, set to None or another appropriate value\n",
    "    if model == '':\n",
    "        model = None\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54f152ef-d9b8-4f7a-8349-8c0432e8bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_with_kde(df, column_name):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x=column_name, bins=30, kde=True)\n",
    "    plt.title(f'Distribution of {column_name.capitalize()}')\n",
    "    plt.xlabel(column_name.capitalize())\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f439d3-68d0-49db-bfc8-d20696fb18d3",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64cfc381-e72e-4580-8921-7efdaa91dfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'dataset.csv'\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22a30b9-f515-40ef-812c-a34f9c84953a",
   "metadata": {},
   "source": [
    "### Data Preparation Function\n",
    "The `prepare_data` function cleans and preprocesses the dataset to ensure it's ready for analysis and modeling.\n",
    "The key operations performed by the function include:\n",
    "1. **Data Cleaning**\n",
    "2. **Removes duplicate**\n",
    "3. **Handling Missing Values**\n",
    "4. **Type conversion variables**\n",
    "5. **Outlier Removal**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6e3f7309-dd9c-4581-92f4-f29c1b690f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "    \n",
    "    #Create a copy of the original data\n",
    "    df = df.copy()\n",
    "    #Remove full duplicates\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    \n",
    "    # Removing the test column with more than 90% missing values ​​without affecting the price column.\n",
    "    # Removing the area column that also has missing values ​​while the city column has more influence on the price.\n",
    "    columns_to_drop = ['Test', 'Area']\n",
    "    # Drop the specified columns\n",
    "    df.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "    # Convert numeric values\n",
    "    df['Km'] = pd.to_numeric(df['Km'], errors='coerce')\n",
    "    df['capacity_Engine'] = pd.to_numeric(df['capacity_Engine'], errors='coerce')\n",
    "    # Merge identical categories\n",
    "    df['Engine_type'] = df['Engine_type'].replace(['היבריד'], 'היברידי')\n",
    "    # Combined the rare categories into 'Other' to simplify the data and reduce noise from infrequent categories.\"\n",
    "    df['Engine_type'] = df['Engine_type'].replace(['גז', 'טורבו דיזל', 'חשמלי'], 'אחר')\n",
    "\n",
    "    # Remove commas from columns\n",
    "    df['Km'] = df['Km'].replace(',', '', regex=True)\n",
    "    df['capacity_Engine'] = df['capacity_Engine'].replace(',', '', regex=True)\n",
    "    \n",
    "    # Handle missing values ​​using groupby and fill in the median, average or common value accordingly\n",
    "    df['capacity_Engine'] = df.groupby('manufactor')['capacity_Engine'].transform(lambda x: x.fillna(x.median()))  \n",
    "    df['Engine_type'] = df.groupby(['model', 'Year'], group_keys=False)['Engine_type'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'אחר'))\n",
    "    df['Km'] = df.groupby('Year')['Km'].transform(lambda x: x.fillna(x.mean())).astype(int)\n",
    "    df = df.dropna(subset=['capacity_Engine'])\n",
    "    \n",
    "    # Ensure capacity_Engine is of integer type\n",
    "    df['capacity_Engine'] = df['capacity_Engine'].astype(int)\n",
    "    \n",
    "    # Handle missing values in categorical columns\n",
    "    mode_color = df['Color'].mode()[0]\n",
    "    df['Color'].fillna(mode_color, inplace=True)\n",
    "    df['Gear'] = df['Gear'].fillna(df['Gear'].mode()[0])\n",
    "    # Fill missing values in 'Prev_ownership' based on 'Curr_ownership'\n",
    "    df['Prev_ownership'].fillna(df['Curr_ownership'], inplace=True)\n",
    "    # Fill missing values in 'Curr_ownership' based on 'Prev_ownership'\n",
    "    df['Curr_ownership'].fillna(df['Prev_ownership'], inplace=True)\n",
    "    # For remaining missing values, fill with the most common value 'פרטית'\n",
    "    df['Prev_ownership'].fillna('פרטית', inplace=True)\n",
    "    df['Curr_ownership'].fillna('פרטית', inplace=True)\n",
    "    median_pic_num = df['Pic_num'].median() \n",
    "    df['Pic_num'].fillna(median_pic_num, inplace=True)\n",
    "    \n",
    "\n",
    "    # Run the function on the 'model' column to handle the values ​​and try to reach a uniform format\n",
    "    df['model'] = df.apply(lambda row: clean_model_column(row['model'], row['manufactor']), axis=1)\n",
    "    df['model'] = df['model'].replace('none', np.nan)\n",
    "    df = df.dropna(subset=['model'])\n",
    "\n",
    "    # Handle missing values ​​in the Supply_score column by the KNN algorithm\n",
    "    num_features = ['Year', 'Hand', 'capacity_Engine', 'Price', 'Km', 'Pic_num']\n",
    "    cat_features = ['Gear', 'Engine_type']\n",
    "    df = fill_supply_score_knn(df, num_features, cat_features)\n",
    "\n",
    "\n",
    "    # Convert categorical columns to categories\n",
    "    df['manufactor'] = df['manufactor'].astype('category')\n",
    "    df['model'] = df['model'].astype('category')\n",
    "    df['Gear'] = df['Gear'].astype('category')\n",
    "    df['Engine_type'] = df['Engine_type'].astype('category')\n",
    "    df['Prev_ownership'] = df['Prev_ownership'].astype('category')\n",
    "    df['Curr_ownership'] = df['Curr_ownership'].astype('category')\n",
    "    df['City'] = df['City'].astype('category')\n",
    "    df['Color'] = df['Color'].astype('category')\n",
    "\n",
    "    # Removes outliers from the dataset by applying filters based on visual analysis (KDE histograms) and personal judgment. \n",
    "    # The goal is to retain only relevant and reasonable data points, ensuring higher data quality for further analysis and modeling.\n",
    "   \n",
    "    df = df[df['Year'] > 2000]\n",
    "    df = df[df['Hand'] <= 6]\n",
    "    df = df[df['Pic_num'] < 10]\n",
    "    df = df[(df['Km'] > 0) & (df['Km'] < 273000)]\n",
    "    df = df[(df['capacity_Engine'] > 150) & (df['capacity_Engine'] < 8000)]\n",
    "    df = df[df['Supply_score'] < 2000]\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "911c5908-e0c2-4f87-b108-b930b542eec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Prepare the data\n",
    "df_processed = prepare_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82097f3-1f79-4f77-bc01-9a4f11de2a83",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472f4e9-6935-4b21-80c9-c1da4ba77c0d",
   "metadata": {},
   "source": [
    "Checking if there are still missing values ​​after running the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b4750782-a23b-4c84-bca4-a278047a64af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "manufactor         0\n",
       "Color              0\n",
       "Description        0\n",
       "Repub_date         0\n",
       "Cre_date           0\n",
       "Pic_num            0\n",
       "Price              0\n",
       "City               0\n",
       "Curr_ownership     0\n",
       "Prev_ownership     0\n",
       "Engine_type        0\n",
       "capacity_Engine    0\n",
       "Gear               0\n",
       "Hand               0\n",
       "model              0\n",
       "Year               0\n",
       "Km                 0\n",
       "Supply_score       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7aefd238-2e01-4ef2-a5b7-3a47f567626e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = df_processed.dropna(subset=['Supply_score'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "74077814-ea67-4900-8f58-f1fb19f0a6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Hand</th>\n",
       "      <th>capacity_Engine</th>\n",
       "      <th>Price</th>\n",
       "      <th>Pic_num</th>\n",
       "      <th>Km</th>\n",
       "      <th>Supply_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1172.000000</td>\n",
       "      <td>1172.000000</td>\n",
       "      <td>1172.000000</td>\n",
       "      <td>1172.000000</td>\n",
       "      <td>1172.000000</td>\n",
       "      <td>1172.000000</td>\n",
       "      <td>1172.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2014.650171</td>\n",
       "      <td>2.233788</td>\n",
       "      <td>1678.891638</td>\n",
       "      <td>51059.929181</td>\n",
       "      <td>2.129693</td>\n",
       "      <td>113647.168089</td>\n",
       "      <td>366.508532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.341197</td>\n",
       "      <td>1.097292</td>\n",
       "      <td>716.983472</td>\n",
       "      <td>22292.222171</td>\n",
       "      <td>2.113009</td>\n",
       "      <td>59780.869260</td>\n",
       "      <td>340.123036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2002.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>18200.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2013.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1200.000000</td>\n",
       "      <td>30475.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>72150.000000</td>\n",
       "      <td>136.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>48000.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>119807.000000</td>\n",
       "      <td>228.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2017.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>68000.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>150460.000000</td>\n",
       "      <td>560.156250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2023.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5500.000000</td>\n",
       "      <td>99960.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>270000.000000</td>\n",
       "      <td>1980.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Year         Hand  capacity_Engine         Price      Pic_num  \\\n",
       "count  1172.000000  1172.000000      1172.000000   1172.000000  1172.000000   \n",
       "mean   2014.650171     2.233788      1678.891638  51059.929181     2.129693   \n",
       "std       3.341197     1.097292       716.983472  22292.222171     2.113009   \n",
       "min    2002.000000     1.000000       800.000000  18200.000000     0.000000   \n",
       "25%    2013.000000     1.000000      1200.000000  30475.000000     1.000000   \n",
       "50%    2015.000000     2.000000      1500.000000  48000.000000     1.000000   \n",
       "75%    2017.000000     3.000000      1800.000000  68000.000000     3.000000   \n",
       "max    2023.000000     6.000000      5500.000000  99960.000000     9.000000   \n",
       "\n",
       "                  Km  Supply_score  \n",
       "count    1172.000000   1172.000000  \n",
       "mean   113647.168089    366.508532  \n",
       "std     59780.869260    340.123036  \n",
       "min        49.000000      0.000000  \n",
       "25%     72150.000000    136.093750  \n",
       "50%    119807.000000    228.125000  \n",
       "75%    150460.000000    560.156250  \n",
       "max    270000.000000   1980.000000  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "524e3df7-8360-4f7f-af9f-060d59265c0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "manufactor         0\n",
       "Color              0\n",
       "Description        0\n",
       "Repub_date         0\n",
       "Cre_date           0\n",
       "Pic_num            0\n",
       "Price              0\n",
       "City               0\n",
       "Curr_ownership     0\n",
       "Prev_ownership     0\n",
       "Engine_type        0\n",
       "capacity_Engine    0\n",
       "Gear               0\n",
       "Hand               0\n",
       "model              0\n",
       "Year               0\n",
       "Km                 0\n",
       "Supply_score       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72359f87-6eda-40a1-8a8b-919dcd6dfbf6",
   "metadata": {},
   "source": [
    "# **--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066fba5f-34d0-45c8-8ff3-b5f104399034",
   "metadata": {},
   "source": [
    "###  I will split the data at this stage before performing normalization to prevent data leakage or bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "187fdfbd-87db-4447-8202-8d9e7f6e9213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = df_processed.drop(columns=['Price'])\n",
    "y = df_processed['Price']\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "d92c6854-f0e9-470f-bdcf-23c17ba1e75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numeric and categorical columns   \n",
    "categorical_features = ['Gear', 'Engine_type', 'manufactor', 'model', 'Prev_ownership', 'Curr_ownership', 'Color','City']\n",
    "numeric_features = ['Year', 'Hand', 'capacity_Engine', 'Km', 'Pic_num', 'Supply_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69923579-ea55-4482-9800-f61c3ada1b62",
   "metadata": {},
   "source": [
    "### Data Preprocessing Steps:\n",
    "This section describes the data preprocessing steps using scikit-learn's `Pipeline` and `ColumnTransformer`:\n",
    "\n",
    "1. **Numerical Features:**\n",
    "   - Impute missing values with the mean.\n",
    "   - Standardize features (mean = 0, std = 1).\n",
    "\n",
    "2. **Categorical Features:**\n",
    "   - Impute missing values with 'most_frequent'.\n",
    "   - One-hot encode categories, ignoring unknowns.\n",
    "\n",
    "3. **Combining Transformations:**\n",
    "   - Use `ColumnTransformer` to apply different transformations to numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "058afa96-4634-47f1-ad98-eac4eabd348d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Define the preprocessing for numerical features with Polynomial Features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False))])  # Adding polynomial features\n",
    "\n",
    "# Define the preprocessing for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent', fill_value='missing')),  # Handle missing values\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "# Combine the transformers into a ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b4ab33-5f86-47a1-84cf-bcd6b5720322",
   "metadata": {},
   "source": [
    " The choice to use polynomial regression stems from its ability to capture non-linear relationships between the independent variables and the dependent variable, which can improve model performance. Using polynomial features also allows for capturing interactions between different features, providing a richer representation of the data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e54668c-84f7-4c76-b855-ad69ce0541b5",
   "metadata": {},
   "source": [
    "### Model and Hyperparameter Tuning\n",
    "\n",
    "In this section, I define and train the ElasticNet regression model. \n",
    "To optimize the model's performance, we use RandomizedSearchCV for hyperparameter tuning, searching over a range of values for the alpha and l1_ratio parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ca7cb516-a5b9-49bc-89be-8c252f70c064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'regressor__alpha': 1.7169933615728772, 'regressor__l1_ratio': 0.9968742518459474}\n",
      "Root Mean Squared Error (RMSE) on training data: 9102.97\n"
     ]
    }
   ],
   "source": [
    "# Define the Elastic Net model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ElasticNet(max_iter=10000, tol=1e-4, random_state=42))])\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "param_distributions = {\n",
    "    'regressor__alpha': uniform(0.01, 20),  # Extended range for Regularization strength\n",
    "    'regressor__l1_ratio': uniform(0, 1)    # Mix between L1 and L2 regularization\n",
    "}\n",
    "\n",
    "# Setup the RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(model, param_distributions, n_iter=500, cv=5, scoring='neg_mean_squared_error', random_state=42)\n",
    "\n",
    "# Fit the RandomizedSearchCV to the data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(\"Best parameters found: \", random_search.best_params_)\n",
    "\n",
    "# Predict on the training data (or you can use validation data if available)\n",
    "y_train_pred = random_search.predict(X_train)\n",
    "\n",
    "# Calculate the RMSE\n",
    "rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "print(f'Root Mean Squared Error (RMSE) on training data: {rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b3a6f-2335-4dd7-bb24-4f959e959593",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe04caae-8510-4c72-9592-da8f7484b9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-Fold Cross-Validation RMSE: [10869.63771705 10948.6540873  13964.81813114 12108.56648101\n",
      " 11074.88008659 12009.44414223 11488.36700983 12430.24286249\n",
      " 13675.61142738 10300.4723855 ]\n",
      "Mean RMSE: 11887.07\n",
      "Standard Deviation of RMSE: 1145.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 10-fold cross-validation\n",
    "cv_scores = cross_val_score(random_search.best_estimator_, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Calculate RMSE for each fold and the mean RMSE\n",
    "rmse_scores = np.sqrt(-cv_scores)\n",
    "mean_rmse = np.mean(rmse_scores)\n",
    "std_rmse = np.std(rmse_scores)\n",
    "\n",
    "print(f'10-Fold Cross-Validation RMSE: {rmse_scores}')\n",
    "print(f'Mean RMSE: {mean_rmse:.2f}')\n",
    "print(f'Standard Deviation of RMSE: {std_rmse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601580d3-4096-4ee7-98ba-fddb8926f9e0",
   "metadata": {},
   "source": [
    "### Model Performance Metrics\n",
    "#### In this section, I evaluate the performance of our trained ElasticNet model using several relevant performance metrics. These metrics provide a comprehensive view of how well the model is performing on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a0e0fc0f-2526-466d-8c71-71a5f7d35a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data: 11067.56\n",
      "Mean Absolute Error (MAE) on test data: 8702.04\n",
      "Adjusted R² Score on test data: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Predict on the test data\n",
    "y_test_pred = random_search.predict(X_test)\n",
    "\n",
    "# Calculate RMSE\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "print(f'Root Mean Squared Error (RMSE) on test data: {test_rmse:.2f}')\n",
    "\n",
    "# Calculate MAE\n",
    "test_mae = mean_absolute_error(y_test, y_test_pred)\n",
    "print(f'Mean Absolute Error (MAE) on test data: {test_mae:.2f}')\n",
    "\n",
    "# Calculate Adjusted R² score\n",
    "n = len(y_test)\n",
    "p = X_test.shape[1]\n",
    "test_adj_r2 = 1 - (1 - test_r2) * (n - 1) / (n - p - 1)\n",
    "print(f'Adjusted R² Score on test data: {test_adj_r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8c0dad-4118-4328-9182-c2723bcd5444",
   "metadata": {},
   "source": [
    "### Identifying the Top 5 Most Influential Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c6dc4bcf-18ce-43a1-82f4-f7fbedf2c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Features with the Largest Impact:\n",
      "\n",
      "+--------------------+---------------------+----------------------+----------+\n",
      "| Feature            | Coefficient         | Absolute Coefficient | Impact   |\n",
      "+--------------------+---------------------+----------------------+----------+\n",
      "| Year               | 16658.294372806315  | 16658.294372806315   | Positive |\n",
      "| model_IMIEV        | 13340.798668160423  | 13340.798668160423   | Positive |\n",
      "| manufactor_מיני    | 12214.194902776027  | 12214.194902776027   | Positive |\n",
      "| model_סונטה        | 11748.931323701776  | 11748.931323701776   | Positive |\n",
      "| model_לנסר ספורטבק | -11081.975579066653 | 11081.975579066653   | Negative |\n",
      "+--------------------+---------------------+----------------------+----------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Extract the model with the best parameters\n",
    "best_model = random_search.best_estimator_.named_steps['regressor']\n",
    "\n",
    "# Get the preprocessor step\n",
    "preprocessor = random_search.best_estimator_.named_steps['preprocessor']\n",
    "\n",
    "# Get numerical feature names after polynomial features\n",
    "numeric_feature_names = preprocessor.transformers_[0][1].named_steps['poly'].get_feature_names_out(numeric_features)\n",
    "\n",
    "# Get categorical feature names\n",
    "categorical_feature_names = preprocessor.transformers_[1][1].get_feature_names_out(categorical_features)\n",
    "\n",
    "# Combine all feature names\n",
    "feature_names = np.concatenate([numeric_feature_names, categorical_feature_names])\n",
    "\n",
    "# Get coefficients\n",
    "coefficients = best_model.coef_\n",
    "\n",
    "# Create a DataFrame to display feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "# Sort by absolute value of coefficients\n",
    "feature_importance['Absolute Coefficient'] = feature_importance['Coefficient'].abs()\n",
    "feature_importance = feature_importance.sort_values(by='Absolute Coefficient', ascending=False)\n",
    "\n",
    "# Add a column for Positive/Negative impact\n",
    "feature_importance['Impact'] = feature_importance['Coefficient'].apply(lambda x: 'Positive' if x > 0 else 'Negative')\n",
    "\n",
    "# Get top 5 features\n",
    "top_5_features = feature_importance.head(5).set_index('Feature')\n",
    "\n",
    "# Align values to the left\n",
    "pd.set_option('display.colheader_justify', 'left')  # for left align\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)  # format floating numbers\n",
    "\n",
    "# Print the top 5 features in a clear and readable format using tabulate\n",
    "print(\"Top 5 Features with the Largest Impact:\\n\")\n",
    "print(tabulate(top_5_features, headers='keys', tablefmt='pretty', numalign='left', stralign='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81aa1eb6-2220-44cd-8874-5b4fea405177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fcb68-7858-4b14-a3a6-f9471322bfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
